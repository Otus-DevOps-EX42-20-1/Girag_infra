# Girag_infra

Girag Infra repository

# ДЗ №3: "Знакомство с облачной инфраструктурой. Google Cloud Platform"

## Способ подключения к `someinternalhost` в одну команду

Создать на рабочем устройстве файл `~/.ssh/config` со следующим содержимым:

```
Host bastion
        HostName 34.77.200.119
        User appuser
        IdentityFile ~/.ssh/appuser

Host someinternalhost
        HostName 10.132.0.3
        User appuser
        IdentityFile ~/.ssh/appuser
        ProxyCommand ssh -W %h:%p bastion
```

Если такой файл уже есть, то добавить эти строки в его конец. После этого можно будет подключаться одной командой `ssh someinternalhost`.

### Дополнительное задание

Добавить в файл `~/.bash_aliases` строку:

```
alias someinternalhost="ssh someinternalhost"
```

И выполнить команду:

```
source ~/.bash_aliases
```

Затем для подключения можно будет использовать команду `someinternalhost`.

# Данные для подключения

bastion_IP = 34.77.200.119
someinternalhost_IP = 10.132.0.3

testapp_IP = 35.205.34.220
testapp_port = 9292

# ДЗ №4

## Команда gcloud для создания инстанса со startup_script

```
gcloud compute instances create reddit-app\
  --boot-disk-size=10GB \
  --image-family ubuntu-1604-lts \
  --image-project=ubuntu-os-cloud \
  --machine-type=g1-small \
  --tags puma-server \
  --restart-on-failure \
  --metadata startup-script='wget -O - https://hastebin.com/raw/cukofawawi | bash'
```

## Команда gcloud для создания правила брандмауэра

```
gcloud compute firewall-rules create default-puma-server --allow=tcp:9292 --target-tags=puma-server
```

# ДЗ №5: "Сборка образов VM при помощи Packer"

Шаблон `ubuntu16.json' был параметризирован с использованием пользовательских переменных. В файле variables.json заданы обязательные переменные. Также в указанный шаблон были добавлены опции для описания образа, размера и типа диска, названия сети и теги. Пример заполнения содержится в файле`variables.json.example`.

### Задания со \*

Создан шаблон `immutable.json`, который "запекает" (bake) в образ VM все зависимости приложения и само приложение. В директории `packer/files` лежит скрипт, который участвует в подготовке образа.
Shell-скрипт `create-reddit- vm.sh` в директории `config-scripts` содержит команду утилиты gloud, которая запустит VM из образа семейства `reddit-full`, подготовленного в рамках ДЗ.

# ДЗ №6: "Практика IaC с использованием Terraform"

Создан главный конфигурационный файл `main.tf`, содержащий декларативное описание нашей инфраструктуры. В файл `outputs.tf` добавлена выходная переменная с IP-адресом VM. В файле `variables.tf` определены входные переменные для параметризации конфигурационных файлов, сами же переменные записаны в файл `terraform.tfvars` – среди них переменная для приватного ключа, использующегося в определении подключения для провижинеров, а также переменная для задания зоны в ресурсе "google_compute_instance" "app". Все конфигурационны файлы отформатированы при помощи команды `terraform fmt`. В файле `terraform.tfvars.example` заданы переменные для образца.

### Задания со \*

В конфигурации Terraform описано добавление SSH-ключей для пользователей appuser1 и appuser2.
В веб-интерфейсе GCP был добавлен SSH-ключ для пользователя appuser_web и затем выполнена команда `terraform apply` – это привело к тому, что добавленный через веб-интерфейс ключ был удалён из VM, так как Terraform привела её конфигурацию к тому виду, который описан в созданной ранее конфигурации.

# ДЗ №7: "Принципы организации инфраструктурного кода и работа над инфраструктурой в команде на примере Terraform"

В директории `packer` создано два новых шаблона `db.json` и `app.json`. При помощи шаблона `db.json` собран образ VM, содержащий установленную MongoDB. Шаблон `app.json` использовался для сборки образа VM с установленным Ruby.
Главный конфигурационный файл `main.tf` был разбит на несколько и созданы окружения `Stage` и `Prod` – каждое со своими конфигурациями и переменными.
Созданы три модуля – `db` (содержит конфигурацию и переменные VM с базой данных), `app` (здесь конфигурация и переменные VM с приложением) и `vpc` (конфигурция и переменные для правил файерволла). Они подключены через главный конфигурационный файл `main.tf` каждого из окружений.
Из реестра модулей установлен модуль `storage-bucket` и был создан тестовый бакет.
Все конфигурационные файлы отформатированы командой `terraform fmt`.

# ДЗ №8: "Управление конфигурацией. Основные DevOps инструменты. Знакомство с Ansible"

Создан инвентори файл `inventory`, в котором сгруппирована информация о созданных инстансах приложения и БД, а также их IP подключения по SSH.
Написан конфигурационный файл `ansible.cfg`, чтобы вынести информацию по умолчанию, например, путь к инвентори файлу, имя пользователя для подключения к серверу по SSH и путь к файлу приватного ключа.
Инвентори файл переписан с использованием YAML и сохранён в `inventory.yml`.
Реализован простой плейбук `clone.yml`, который выполняет клонирование репозитория с приложением. После выполнения команд `ansible-playbook clone.yml`, а затем `ansible app -m command -a 'rm -rf ~/reddit'` повторное исполнение плейбука изменило вывод с `changed=0` на `changed=1` – это произошло из-за предварительного физического удаления директории `~/reddit`, в которую клонировался репозиторий. Соответственно, эта директория появилась на сервере вновь, что и отразилось в единственном changed.

# ДЗ №9: "Деплой и управление конфигурацией с Ansible"

Создан плейбук `reddit_app.yml` для управления конфигурацией и деплоя нашего приложения.
Использован модуль `template`, чтобы скопировать параметризированный конфиг файл MongoDB на удалённый хост.
Для каждого таска в плейбуке задан свой тег, чтобы иметь возможность запуска по тегам отдельных тасков, а не весь плейбук сразу.
Написан шаблон `mongod.conf.j2`, содержащий параметризованный конфиг MongoDB для управления адресом и портом сервера БД.
Добавлены хендлеры для рестарта БД при изменении конфигурационного файла, а также повторного чтения изменённого unit для systemd.
В плейбук добавлен таск для копирования unit-файла на инстанс с приложением. Использованы модули `copy` (для просто копирования файлов на удалённый хост) и `systemd` (для настройки автостарта Puma-сервера).
Создан новый плейбук `reddit_app2.yml` с несколькими сценариями – отдельно для конфигурирования БД, приложения и деплоя.
В итоге для лучшей читаемости было решено разделить большие плейбуки на несколько маленьких – `app.yml`, `db.yml` и `deploy.yml`, а старые переименовать в `reddit_app_one_play.yml` и `reddit_app_multiple_plays.yml`.
Был изменён provision в Packer с bash-скриптов на Ansible-плейбуки (`packer_db.yml` и `packer_app.yml`).

# ДЗ №10: "Ansible: работа с ролями и окружениями"

Создана директория `roles`, в которой были выполнены команды `ansible-galaxy init app` и `ansible-galaxy init db` – они создали заготовки ролей для конфигурации нашего приложения и БД.
Секции `tasks` из плейбуков `app.yml` и `db.yml` перенесены в файлы `roles/app/tasks/main.yml` и `roles/db/tasks/main.yml` соответственно. Так как в ролях модули `template` и `copy` по умолчанию проверяют наличие шаблонов и файлов в директориях `templates` и `files`, то в тасках были оставлены только имена шаблонов и файлов для `src`.
Шаблонизированный конфиг для MongoDB перенесён из директории `templates` в `roles/db/templates`, unit-файл – из `files` в `roles/app/files`.
Хендлеры из плейбуков перемещены в файлы `handlers/main.yml` соответствующих ролей. В бывших плейбуках `app.yml` и `db.yml` теперь вызываются роли.
Создали два окружения `environments/prod` и `environments/stage` с отдельными инвентори файлами для каждого из них.
В конфиге Ansible `ansible.cfg` задали `stage` окружением по умолчанию.
Для параметризации конфигурации ролей создали директории `environments/stage/group_vars` и `environments/prod/group_vars`. В файле `group_vars/app` определили переменные для группы хостов `app`, а в `group_vars/db` для `db`. В `group_vars/all` содержатся переменные для группы `all`, представляющей все хосты инвентори файла.
В файлах `defaults/main.yml` определена переменная по умолчанию `env`.
При помощи модуля `debug` в начало ролей был добавлен таск с выводом текущего окружения конфигурируемого хоста.
Все плейбуки перенесены в директорию `playbooks`, а неактуальное в директорию `old`. Улучшен конфиг Ansible `ansible.cfg`.
Установлена community-роль `jdauphant.nginx`, с помощью которой на поднимается веб-сервер nginx, работающий в качестве прокси, что сделало возможным работу нашего приложения на 80 порту.
Добавлен плейбук `playbooks/users.yml` для создания пользователей. При помощи команды `ansible-vault encrypt` и предварительно подготовленного файла ключа `vault.key` были зашифрованы файлы с данными пользователей для каждого окружеия.

# ДЗ №11: "Разработка и тестирование Ansible ролей и плейбуков"

Были установлены `VirtualBox` (как один из провайдеров, которым может управлять `Vagrant`) и сам `Vagrant`.
Создан `Vagrantfile` с описанием двух VM, которые мы хотим создать локально. В качестве провижинера для Vagrant используем Ansible с созданными ранее ролями и плейбуками.
Написан плейбук `playbooks/base.yml` для установки `Python` на управляемом хосте.
Доработали роли `db` и `app`, разделив таски по разным файлам и сделав вызов их в главных файлах `main.yml` в нужном порядке.
Произведена параметризация конфигурации и unit-файла для сервера Puma.
Дополнена конфигурация `Vagrant` для корректной работы проксирования приложения с помощью nginx.
Установлены `Molecule` для создания машин и проверки конфигурации, а также `Testinfra` для написания тестов.
С помощью команды `molecule init` созданы заготовки тестов для роли `db`, а позже добавлено несколько тестов для проверки (в том числе для проверки того, что БД слушает нужный порт).
Скорректировали плейбуки `packer_db.yml` и `packer_app.yml` для использования в них ролей `db` и `app`.
